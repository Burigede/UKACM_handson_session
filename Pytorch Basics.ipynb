{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df00a7c7-ad6e-4711-a7ac-e90906e6a104",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Python and Pytorch basics\n",
    "\n",
    "## oerview of today's lecture \n",
    "    1. Tensor basics: \n",
    "        - Create, Operations, Numpy\n",
    "    2. Autograd: \n",
    "        - Linear regression example\n",
    "    3. Training loop:\n",
    "        - Model, Loss, Optimizer and Scheduler\n",
    "    4. Nueral network: \n",
    "        - Datasets, DataLoader, GPU (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b3bce-1fa1-4974-abec-4a66908fd182",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 1. Tensors \n",
    "\n",
    "### Definition: \n",
    "A multidimenisonal matrix containing a single data type. \n",
    "\n",
    "Tensors are simply mathematical objects that can be used to describe <font color='red'> physical properties </font>, just like scalars and vectors. In fact tensors are merely a generalisation of scalars and vectors; a scalar is a zero rank tensor, and a vector is a first rank tensor. \n",
    "\n",
    "### Example - Hook's law in 3D: \n",
    "$$\\sigma_{ij} = C_{ijkl}\\epsilon_{kl}$$ \n",
    "    -$\\sigma, \\epsilon \\in \\mathbb{R}^{3 \\times 3}$ are <font color='red'>second</font> order tensors.  \n",
    "    -$C \\in \\mathbb{R}^{3 \\times 3 \\times 3 \\times 3}$ is a <font color='red'>fourth</font> order tensor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d7c3e-bc44-4426-9320-fc9131ce258b",
   "metadata": {},
   "source": [
    "## Pytorch Tensors\n",
    "Everything in Pytorch is based on tensors.  \n",
    "\n",
    "#### Use <font color='red'>torch.tensor()</font> to generate tensors with known entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7899b8-b82a-4b68-9746-b85a04128738",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import torch # This is the first line you should write in your Pytorch script \n",
    "import numpy as np\n",
    "\n",
    "# Create Pytorch tensor for scalar 1\n",
    "a = torch.tensor([1.]) \n",
    "print(\"a = \", a)\n",
    "\n",
    "# Create pytorch tensor for vector [1,-0.5]\n",
    "b = torch.tensor([1.0,-0.5])\n",
    "print(\"b = \", b) \n",
    "\n",
    "# Create Pytorch tensor for matrix [1,-0.5,-0.5,1]\n",
    "c = torch.tensor([[1.,-0.5],[-0.5,1.]])\n",
    "print(\"c = \", c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc0d3b-5665-498b-b73c-8964047af19b",
   "metadata": {},
   "source": [
    "### Use <font color='red'>torch.rand()</font> to generate tensors with random entries between (0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2d06c-8ca2-4fb8-b003-39d2ec14a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also generate random numbers  \n",
    "x = torch.rand(2,2,3) # tensor, 3 dimensional\n",
    "print (x) \n",
    "\n",
    "y = torch.rand(2,3,4,5) # tensor, 4 dimensional\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93fdfc-6ab7-4809-9095-e4b5ac6b8a03",
   "metadata": {},
   "source": [
    "#### We can use <font color='red'>torch.ones()</font> to generate constant tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf45c2b-cf49-4dab-8f49-3c4413bc547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant second order tensor of 1\n",
    "x = torch.ones(2,2)\n",
    "print(x)\n",
    "\n",
    "# Constant third order tensor of 1.5\n",
    "x = torch.ones(2,2,2)*1.5\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f78e6-5104-4a4b-8b95-8ba5a2f6326f",
   "metadata": {},
   "source": [
    "#### Slicing of tensor entries is similar to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a1a70-c2e0-4c1b-9dd0-5adb7f2331d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,3)\n",
    "print(x)\n",
    "# To access the first row\n",
    "a = x[0,:]\n",
    "print(a)\n",
    "# To access the first column\n",
    "b = x[:,0]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb438b7-2e45-40c5-98f4-26b526b7e67a",
   "metadata": {},
   "source": [
    "#### Check size of tensors with <font color='red'>tensor.shape</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87660381-5b50-499b-a659-838f16854c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,4)\n",
    "print(a.shape)\n",
    "# You can also specify the dimension of the tensor that you wish to query\n",
    "print(a.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b397f5-5fc8-409e-bbf4-fa7022d4e059",
   "metadata": {},
   "source": [
    "#### Check tensor type with <font color='red'>tensor.dtype</font> - default float32 (single precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f90031-8a63-48b9-8f88-53871db83a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,4)\n",
    "print(a.dtype)\n",
    "b = torch.tensor([0,1])  \n",
    "print(b.dtype)\n",
    "c = torch.tensor([0.,1.])  \n",
    "print(c.dtype)\n",
    "d = torch.tensor([0,1], dtype=torch.float64)\n",
    "print(d.dtype)\n",
    "\n",
    "d = c.double()# this changes the data type to double precision\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44ad34-cff2-4ef7-b73b-1029194774ad",
   "metadata": {},
   "source": [
    "#### Reshape tensors with <font color='red'>torch.view()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6e39d-46df-4ac3-9037-546ef08d9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2,2)\n",
    "# Reshape a 2x2 matrix into a 4x1 vector\n",
    "b = a.view(4,1) \n",
    "print(b) \n",
    "print(b.shape) \n",
    "# You can put -1 in one dimension, to force Pytorch to infer from the rest dimensions\n",
    "c = a.view(-1,2)\n",
    "d = a.view(-1,4)\n",
    "\n",
    "print(c.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33170d28-8ee5-4c98-af3c-dbd9b4823c9c",
   "metadata": {},
   "source": [
    "## Tensor Linear Algebra \n",
    "#### Adding two tensors <font color='red'>torch.add()</font> or simply <font color='red'>+</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5105dc-d1a6-4bc7-94f3-ce9109db1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.ones(2,2)\n",
    "\n",
    "c = a+b \n",
    "print(c)\n",
    "d = torch.add(a,b)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84956f13-cee9-4237-b9f9-f1c7ba2d25e4",
   "metadata": {},
   "source": [
    "### Similarly Substraction <font color='red'>\"-\"</font> Multiplication <font color='red'>\"*\"</font> Division <font color='red'>\"/\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fb082-b920-42c0-ade2-04d93c9f38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = torch.ones(2,2)+1\n",
    "\n",
    "print(\"a-b = \", a-b)\n",
    "print(\"a*b\", a*b)\n",
    "print(\"a/b\", a/b)  # Note this is element wise division! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa87c5-e43f-41dd-b605-debea09260fc",
   "metadata": {},
   "source": [
    "#### Converting torch.tensor to Numpy array and vice versa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300af7a3-5486-443d-9131-2e665f0454fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is important for exmaple in visualization - Matplotlib does not accept tensor type\n",
    "a = torch.ones(2,2)\n",
    "b = a.numpy()\n",
    "print(b) \n",
    "print(type(b))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f2008-993e-4d99-916d-2138bf0220d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do the inverse, just put torch.tensor(nparray)\n",
    "a = np.eye(3)\n",
    "b = torch.tensor(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560a101-72a2-41ed-bcc8-0d17c6096686",
   "metadata": {},
   "source": [
    "## Autograd \n",
    "### Finding function derivatives \n",
    "Autograd is a key enabler of Pytorch. The package uses automatic differentiation for all operations on Tensors. Generally speaking,<font color='red'>\"torch.autograd\"</font> is an engine for computing the vector-Jacobian product. It computes partial derivates while applying the chain rule. \n",
    "\n",
    "Let us consider the function \n",
    "$$y = 2x + 1\\quad for \\  x \\in [0,1]$$\n",
    "Then\n",
    "$$\\frac{dy}{dx} = 2 $$\n",
    "\n",
    "Let us now use <font color='red'>\"torch.autograd\"</font> to find this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf326b-0b92-484b-8a36-ae1af089f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.linspace(0,1,10,requires_grad=True)\n",
    "y = 2*x + 1\n",
    "dydx = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y),create_graph=True)[0]  \n",
    "plt.plot(x.detach().numpy(),y.detach().numpy()) # You need to detach the tensor first and convert it to numpy for plotting\n",
    "plt.plot(x.detach().numpy(),dydx.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6f449-b1f0-472e-9fad-f1f3cdeb9fc3",
   "metadata": {},
   "source": [
    "Now suppose we further define\n",
    "$$z = y^2 $$\n",
    "and \n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} =2y \\cdot 2 = 8x+4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021dc389-7e55-4908-8b58-40ed3f1ce33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,1,10,requires_grad=True)\n",
    "y = 2*x + 1\n",
    "z = y**2 \n",
    "dzdx = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(y),create_graph=True)[0]  \n",
    "plt.plot(x.detach().numpy(),dzdx.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77ee4c-26f2-415d-aee4-53762ce09208",
   "metadata": {},
   "source": [
    "### Back propagation with <font color='red'>\"scalar_tensor.backward()\"</font>\n",
    "A convenient way of computing the derivatives of a <font color='red'>scalar</font> function with respect to inputs. Commonly used to compute the back propagation of Loss functions. Note here Loss function is a scalar function by default. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d24c48-28b3-4c93-995a-f7676bc5023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the scalar function w.r.t. the input tensor\n",
    "\n",
    "x = torch.tensor([0., 0.5, 1],requires_grad=True)\n",
    "y = x + 2 \n",
    "z = torch.sum(y) # z a scalar function\n",
    "\n",
    "print(x.grad)\n",
    "z.backward() \n",
    "print(x.grad) # dz/dx  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb801c9-fed8-40e2-a4f2-2e5c00d2f17d",
   "metadata": {},
   "source": [
    "### <font color='red'>Note: backward() accumulates the gradient for this tensor into .grad attribute </font> \n",
    "### <font color='red'>Use optimizer.zero_grad() to clear the gradient in optimization! </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63c11c-1fab-4853-a8b0-ed389bc998d8",
   "metadata": {},
   "source": [
    "#### Stop a tensor from tracking history:\n",
    "For example during the training loop when we want to update our weights, or after training during evaluation. These operations should not be part of the gradient computation. To prevent this, we can use:\n",
    "\n",
    "- `x.requires_grad_(False)`\n",
    "- `x.detach()`\n",
    "- wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba835c4a-2477-4956-b4cb-8d5c35f8a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "a = torch.randn(2, 2)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7a47e-1b98-40c1-9900-2968e0391e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad_(True)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e84b7-e576-434e-9052-076a5e446e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "b = a.detach()\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128999a-5359-4b98-add2-0e9597aa69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    b = a ** 2\n",
    "    print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f703283-08c7-4da9-ae0d-988be633ce94",
   "metadata": {},
   "source": [
    "### Gradient Descent Method\n",
    "Linear Regression example with mean squared loss function:\n",
    "\n",
    "$f(x) = w * x + b$\n",
    "\n",
    "here  $$\\hat{f}(x) = 2*x$$\n",
    "\n",
    "$$loss(f,\\hat{f}) = \\frac{1}{N_x} \\sum_{k=0}^{N_x}(f(x_k) - \\hat{f}(x_k))^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec5b5b-2ac6-4691-a519-7d120b65e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction model class\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "# Define the loss function\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f0b39-47f3-4fb7-b5d8-8c32ab1890e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "X_test = 5.0\n",
    "\n",
    "print(f'Prediction before training: f({X_test}) = {forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0337b-c2a9-4828-83ac-d0be70d2e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "learning_rate = 0.01 # Important hyper parameter! \n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "      w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.3f}')\n",
    "\n",
    "print(f'Prediction after training: f({X_test}) = {forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0d4fe-c079-4dcb-8fb3-f26d3547c5f4",
   "metadata": {},
   "source": [
    "### 3. Model, Loss & Optimizer\n",
    "\n",
    "A typical PyTorch pipeline looks like this:\n",
    "\n",
    "1. Design model (input, output, forward pass with different layers)\n",
    "2. Specify train and test data \n",
    "3. Construct loss and optimizer\n",
    "4. Training loop:\n",
    "      - Forward = compute prediction and loss\n",
    "      - Backward = compute gradients\n",
    "      - Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f8fc7-bb28-4620-968c-7269a829c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "# here : f = 2 * x\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "\n",
    "# Here we could simply use a built-in model from PyTorch\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define different layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f433bd-f63a-4f44-be68-7a6f918ce8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Specify training samples, watch the shape! \n",
    "X = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)\n",
    "\n",
    "# 2) A general way of specifying train/test data is to reshape the data as N_sample x N_feature1 x N_feature2 ....\n",
    "n_samples, n_features = X.shape\n",
    "print(f'n_samples = {n_samples}, n_features = {n_features}')\n",
    "\n",
    "# 2) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32) \n",
    "\n",
    "# 2) Initialize model \n",
    "input_size, output_size = n_features, n_features\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f({X_test.item()}) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21744ce-3f8d-440d-a47c-172fef52e7a6",
   "metadata": {},
   "source": [
    "#### Select your optimizer with <font color='red'>torch.optim</font>, and be careful with your learning rate \n",
    "#### Tune your learning rate during the training with <font color='red'>torch.optim.lr_scheduler</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4a055-50a6-4d2f-8d30-4683c2247125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) # Decrease learning rate by 10 every 30 training epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eeb690-2d12-4364-8004-7dc2261ae55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        w, b = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l.item())\n",
    "\n",
    "print(f'Prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474dcdf",
   "metadata": {},
   "source": [
    "## 4. First Neural Net\n",
    "Datasets, DataLoader, Neural Net, Training & Evaluation, GPU(optional)\n",
    "\n",
    "### Datasets: Let us consider the celebrated MNIST data set for ML \n",
    "\n",
    "### <font color='red'>Don't forget to normalize your data before training!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration (Optional)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 784 # 28x28\n",
    "output_classes = 10\n",
    "FCNN_arch = [input_size, 500,output_classes]\n",
    "non_linearity = nn.ReLU \n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loading the MNIST dataset  600 training data, 100 test data\n",
    "# Generally you need to define this your self\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd33f8f",
   "metadata": {},
   "source": [
    "### Design your neural network architecture and choose <font color='red'>  nonlinearity</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network w\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, layers, nonlinearity):\n",
    "        super(FCNN, self).__init__()\n",
    "\n",
    "        self.n_layers = len(layers) - 1\n",
    "\n",
    "        assert self.n_layers >= 1\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for j in range(self.n_layers):\n",
    "            self.layers.append(nn.Linear(layers[j], layers[j + 1]))\n",
    "\n",
    "            if j != self.n_layers - 1:\n",
    "                self.layers.append(nonlinearity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "\n",
    "        return x \n",
    "    \n",
    "FCNN_arch = [input_size, 500,output_classes] \n",
    "non_linearity = nn.ReLU \n",
    "model = FCNN(FCNN_arch, non_linearity).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb317f6",
   "metadata": {},
   "source": [
    "### Loss and optimizer: this is a classification problem - use cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) # Decrease learning rate by 10 every 30 training epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f874b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass and loss calculation\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{600}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    # Test the model: we don't need to compute gradients\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = len(test_loader.dataset)\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # max returns (output_value ,index)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = n_correct / n_samples    \n",
    "    print (\"end epoch\", epoch, \"Train loss\", loss.item(), \"Accuracy\", acc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
